---
title:  Recognizing Korean Food in Photos through Transfer Learning of Deep Features

date: March 7, 2016

author: Joseph Kim

bibliography: references.bib

geometry: left=1in,right=1in,top=1in,bottom=1in

csl: ieee-with-url.csl
...


# Introduction

Food is one of the most common things that people take photos of these days. Google reported that food was the second most common photo taken by users of Google Photos [@GoogleBlog:Photos]. Given that there is so much data, as well as the fact that there are so many food-based startups these days, applying image recognition on photos of food has lots of potential for fun applications as well as commercial applications, such as recommender systems. A quick google search shows that there are a few academic food recognition projects and papers, but none particularly for Korean foods.

The goal of this project was to create a Korean food recognizer that can identify Korean foods in photos. Specifically, the goal was to create a supervised machine learning model trained on images of Korean foods that takes a new image as input, and accurately output the name of the dish in the photo. Because of the success of deep convolutional neural networks (CNN) for image recognition in recent years, an assumption was made that the deep features generated by the CNNs in the second-last layer of the CNNs would be a good "embedding" of each image, without the need for feature engineering. By applying transfer learning, all that was left was to retrain the top layer of a pre-trained CNN to recognize Korean food in photos.

Google's Inception-v3 CNN was a very easy choice for a pre-trained network, which had been trained on ILSVRC2014 dataset to perform very well on the ImageNet Large Visual Recognition Challenge. At the time of this writing, it outperforms all the other models for the ImageNet dataset [@SzegedyVISW15]. The ILSVRC dataset contains 1.2 million images from 1000 categories [@ILSVRC15], which means that Inception v3 was a good choice for a generic image recognition tool that I can fine-tune for my problem.

In order to set a benchmark performance, other machine learning tools and algorithms were used on the deep features. Additionally, another CNN implemented by a private company, MetaMind, was used for comparison.


## Metrics

For this supervised multi-class classification problem, the performance of the models were primarily measured by *accuracy* on the test data, as defined by the following formula:
$$accuracy = \frac{ TP | TN }{TP | FP | FN | TN}$$
$TP$ is true positives, $TN$ is true negatives, $FP$ is false positives, and $TN$ is true negatives.

For training the last layer of the deep convolutional network, the objective loss function is *cross entropy* (also known as $log loss$) defined as:
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^Ky_{ij}\log(p_{ij})$$
$N$ is the number of training samples, $K$ is the number of classes, which is the number of Korean food dishes in this project, $y_{ij}$ is the correct binary value (1 if it is that label, 0 otherwise) for the $jth$ label for sample $i$, and $p_{ij}$ is the predicted probability of sample $i$ for the $jth$ label.

*F1 score* was also considered for analysis, but because the distribution of training data across each of the labels was not overly skewed (See Figure \ref{figure_data_distrib}), *accuracy* was good enough to gauge performance for this multi-classification problem.


# Analysis

## Data

### Acquiring and Filtering
20 Korean dishes were selected based on how common and popular they are among Korean and Korean Americans.
For each of the dishes, 150-300 images were downloaded from the web using Google image search, for a total of around 5,000 images.
Then each of the images were manually viewed, and filtered out if the images were low quality or deemed non-representative of the actual dish.
After the filtering process, the dataset consisted of 3,470 images, with max, mean and min of 219, 173.5 and 112 images per label, respectively.

| Name           |  Number of samples
| -----------		 | ------------------
| galbijjim      |  219
| kimbab         |  217
| bibimbab       |  214
| hotteok        |  210
| nangmyun       |  207
| dakgalbi       |  205
| sullungtang    |  193
| japchae        |  193
| bulgogi        |  183
| samgyupsal     |  173
| bossam         |  173
| dakbokeumtang  |  171
| jeyookbokkeum  |  165
| samgyetang     |  158
| ddukbokee      |  150
| lagalbi        |  148
| jeon           |  134
| kimchi         |  127
| ramen          |  118
| yookgyejang    |  112

Table: (20 Korean Food Categories Counts) \label{table_data_distrib}

![Data Distribution\label{figure_data_distrib}](report_images/data_distrib.png "Korean Food Data Distribution")


### Deep Features Transformation

Each image file was transformed to deep features, which are numeric embedding vectors of size 2048 using a modified version of the `classify_image.py` script provided by Google in the TensorFlow source code and saved to disk for further analysis. The embedding is the value of the tensor in the second-last layer of the Inception-v3 Deep Convolutional Network.

|       |  feature0	    |  feature1	    |  feature2	    | feature3	   | feature4
|-------|---------------|---------------|---------------|--------------| -----------
|count	|  3470.000000  |  3470.000000	|  3470.000000	| 3470.000000	 | 3470.000000
|mean	  |  0.292122	    |  0.409072	    |  0.527338	    | 0.205257	   | 0.341956
|std	  |  0.210247	    |  0.310620	    |  0.348090	    | 0.196429	   | 0.309863
|min	  |  0.002776	    |  0.000000	    |  0.009342	    | 0.000000	   | 0.000000
|25%	  |  0.135991	    |  0.177316	    |  0.271595	    | 0.066020	   | 0.121058
|50%	  |  0.244641	    |  0.341317	    |  0.450077	    | 0.147333	   | 0.259115
|75%	  |  0.396790	    |  0.555601	    |  0.708050	    | 0.281080	   | 0.469292
|max	  |  1.751740	    |  2.421970	    |  2.682040	    | 1.811220	   | 3.519750

Table: (Description of First Five Deep Features) \label{table_deep_features}

![Description of First Five Deep Features \label{figure_deep_features_description}](report_images/deep_features_sample.png "First Five Deep Features")

The values of the deep features were similar to each other in terms of range, and there were no concerns about certain features dominating other features with values that were much bigger. In other words, standardizing columns was not a concern here.

### Train-Test Split

The dataset was then split into separate train and test sets with the training set consisting of 70% (2431 samples) of the data, and the test set 30% (1039 samples). The splits were stratified with respect to the labels, so that the representation of the labels after the split remain similar to the dataset before the split.


|               | train | test | % test  |
|---------------|-------|------|---------|
|    bibimbab   |  150  |  64  | 29.9    |
|     bossam    |  121  |  52  | 30.0    |
|    bulgogi    |  128  |  55  | 30.0    |
| dakbokeumtang |  120  |  51  | 29.8    |
|    dakgalbi   |  144  |  61  | 29.7    |
|   ddukbokee   |  105  |  45  | 30      |
|   galbijjim   |  153  |  66  | 30.1    |
|    hotteok    |  147  |  63  | 30      |
|    japchae    |  135  |  58  | 30.0    |
|      jeon     |   94  |  40  | 29.8    |
| jeyookbokkeum |  116  |  49  | 29.6    |
|     kimbab    |  152  |  65  | 29.9    |
|     kimchi    |   89  |  38  | 29.9    |
|    lagalbi    |  104  |  44  | 29.7    |
|    nangmyun   |  145  |  62  | 29.9    |
|     ramen     |   83  |  35  | 29.6    |
|   samgyetang  |  111  |  47  | 29.7    |
|   samgyupsal  |  121  |  52  | 30.0    |
|  sullungtang  |  135  |  58  | 30.0    |
|  yookgyejang  |   78  |  34  | 30.3    |

Table: (Stratified Train-Test Split) \label{table_train_test_split}



## Algorithms

### Transfer Learning

Transfer learning in machine learning is the application of knowledge learned in one problem to another similar or related problem, and has been around for at least a couple of decades [@wiki:transfer]. A relatively recent discovery of transfer learning through trained deep neural networks has raised a lot of excitement and opened doors to a lot of applications. Researchers found that the trained weights of nodes in a trained network, particularly the lower and mid-level nodes, can be copied readily to other deep networks, along with the structure of the deep network, and just retraining the high-level classification layer can result in a network that performs very well for a new problem. For example, for image recognition, taking a convolutional neural network that performs very well on recognizing various types of images and retraining the classification layer to identify a specific set of labels such as dog breeds or food types can work well. That's because the lower nodes and layers of the CNN learn low-level features of the images, which are necessary for many image recognition tasks, but the higher nodes and layers learn high-level features, which tend to be more specific to the dataset [@YosinskiCBL14].

The Inception-v3 network was used to generate 2048 deep features, and the top fully-connected layer of the network was retrained, which applies the Softmax function to output probability values for each of the Korean food labels.

### Benchmark

#### Non-Neural Network Algorithms

Preliminary benchmarks were set using six different kinds of machine learning classifier algorithms, and 3 highest performing algorithms were fine-tuned to set a benchmark to beat.

| Classifer Types
| ---
| Support Vector Classifier
| K-Neighbors Classifier
| Random Forest Classifier
| Logistic Regression
| Gradient Boosting Classifier
| AdaBoost Classifier

Table: (Stratified Classifier Types) \label{table_classifier_types}

The initial performances of the non-neural network algorithms are described in Table \ref{table_preliminary_non_nn}.

|                                Algorithm   | Train Accuracy  | Test Accuracy  
|--------------------------------------------|-----------------|----------------
|          **SVC linear kernel, one-vs-one** |       1.000000  |  **0.794995**  
|    SVC polynomial deg 2 kernel, one-vs-one |       0.266557  |      0.271415  
|    SVC polynomial deg 3 kernel, one-vs-one |       0.063348  |      0.064485  
|                 SVC rbf kernel, one-vs-one |       0.763472  |      0.704524  
|             SVC linear kernel, one-vs-rest |       1.000000  |      0.769971  
|  SVC linear poly deg 2 kernel, one-vs-rest |       0.839572  |      0.721848  
|  SVC linear poly deg 3 kernel, one-vs-rest |       0.868367  |      0.726660  
|                SVC rbf kernel, one-vs-rest |       0.830111  |      0.730510  
|                    KNeighbors, 3 neighbors |       0.815714  |      0.619827  
|                RandomForest, 10 estimators |       0.996709  |      0.457170  
|                 **LogisticRegression ovr** |       1.000000  |  **0.793070**  
|         **LogisticRegression multinomial** |       1.000000  |  **0.803657**  

Table: (Preliminary Benchmark of Non-Neural Network Algorithms) \label{table_preliminary_non_nn}

The 3 highest preforming algorithms from Table \ref{table_preliminary_non_nn} that were selected for fine-tuning were SVC (Support Vector Classifier) with a linear kernel, Logistic Regression using multiple one-vs-rest classifiers, and Logistic Regression using a single multinomial model.


#### MetaMind

MetaMind, a private startup specializing in deep learning services, provides limited free services image classification. Though there isn't much documentation about the specific algorithms used on the site, based on my readings and what I know of their CEO, Richard Socher, I deduced that their image recognition service uses transfer learning of deep convolutional networks.



# Methods


# Results


# Discussion

A big advantage of transfer learning is that we can drastically reduce the total amount of computations needed to train deep networks. Successful deep learning projects are often trained on high performance clusters and servers with GPUS to reduce the time to compute all the vectorized matrix operations, and runtimes can be hours, days and even weeks. With only a personal laptop, an engineer can take one of these trained networks, and just retrain the last layer of the network on a smaller set of data for a specific classification task without performance or memory issues.

A disadvantage of transfer learning is that the resulting classifier model may not be able to outperform a CNN whose entire set of weights are retrained at all levels for the specific problem, not just the upper layers. A good way to leverage a trained CNN would be to take the weights and use them as initial values, but keep them variable so that the model can be fine-tuned for the specific problem. As long as there is enough good-quality training data that won't cause the new CNN to overfit to the training set, retraining the entire network may be a better idea if the computing resources are available.



# References
